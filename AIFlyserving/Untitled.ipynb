{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ui.py\n"
     ]
    }
   ],
   "source": [
    "%%file ui.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "#import argcomplete\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from functools import wraps\n",
    "import numpy as np\n",
    "from flask import Flask, Response, jsonify, render_template, request,redirect,url_for,session\n",
    "from PIL import Image\n",
    "from flask_cors import CORS\n",
    "\n",
    "from redis_test import RedisDict\n",
    "from model_loader import ModelLoader\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Define parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# TODO: Remove if it does not need gunicorn\n",
    "parser.add_argument(\n",
    "    \"--bind\",\n",
    "    default=\"0.0.0.0:8500\",\n",
    "    help=\"Bind address of the server(eg. 0.0.0.0:8500)\")\n",
    "parser.add_argument(\n",
    "    \"--host\", default=\"0.0.0.0\", help=\"The host of the server(eg. 0.0.0.0)\")\n",
    "parser.add_argument(\n",
    "    \"--port\", default=8500, help=\"The port of the server(eg. 8500)\", type=int)\n",
    "parser.add_argument(\n",
    "    \"--model_name\",\n",
    "    default=\"default\",\n",
    "    help=\"The name of the model(eg. default)\")\n",
    "parser.add_argument(\n",
    "    \"--model_base_path\",\n",
    "    default=\"./model\",\n",
    "    help=\"The file path of the model(eg. 8500)\")\n",
    "parser.add_argument(\n",
    "    \"--model_platform\",\n",
    "    default=\"tensorflow\",\n",
    "    help=\"The platform of model(eg. tensorflow)\")\n",
    "parser.add_argument(\n",
    "    \"--model_config_file\",\n",
    "    default=\"\",\n",
    "    help=\"The file of the model config(eg. '')\")\n",
    "# TODO: type=bool not works, make it true by default if fixing exit bug\n",
    "parser.add_argument(\n",
    "    \"--reload_models\", default=\"False\", help=\"Reload models or not(eg. True)\")\n",
    "parser.add_argument(\n",
    "    \"--custom_op_paths\",\n",
    "    default=\"\",\n",
    "    help=\"The path of custom op so files(eg. ./)\")\n",
    "parser.add_argument(\n",
    "    \"--verbose\",\n",
    "    default=True,\n",
    "    help=\"Enable verbose log or not(eg. True)\",\n",
    "    type=bool)\n",
    "parser.add_argument(\n",
    "    \"--gen_client\", default=\"\", help=\"Generate the client code(eg. python)\")\n",
    "parser.add_argument(\n",
    "    \"--enable_auth\",\n",
    "    default=False,\n",
    "    help=\"Enable basic auth or not(eg. False)\",\n",
    "    type=bool)\n",
    "parser.add_argument(\n",
    "    \"--auth_username\",\n",
    "    default=\"admin\",\n",
    "    help=\"The username of basic auth(eg. admin)\")\n",
    "parser.add_argument(\n",
    "    \"--auth_password\",\n",
    "    default=\"admin\",\n",
    "    help=\"The password of basic auth(eg. admin)\")\n",
    "parser.add_argument(\n",
    "    \"--enable_colored_log\",\n",
    "    default=False,\n",
    "    help=\"Enable colored log(eg. False)\",\n",
    "    type=bool)\n",
    "parser.add_argument(\n",
    "        \"--enable_cors\",\n",
    "        default=True,\n",
    "        help=\"Enable cors(eg. True)\",\n",
    "        type=bool)\n",
    "\n",
    "# TODO: Support auto-complete\n",
    "#argcomplete.autocomplete(parser)\n",
    "\n",
    "if len(sys.argv) == 1:\n",
    "    args = parser.parse_args([\"-h\"])\n",
    "    args.func(args)\n",
    "else:\n",
    "    args = parser.parse_args(sys.argv[1:])\n",
    "    for arg in vars(args):\n",
    "        logging.info(\"{}: {}\".format(arg, getattr(args, arg)))\n",
    "\n",
    "    if args.enable_colored_log:\n",
    "        import coloredlogs\n",
    "        coloredlogs.install()\n",
    "\n",
    "# TODO: Support auto-complete\n",
    "#argcomplete.autocomplete(parser)\n",
    "\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "\n",
    "class InferenceService(object):\n",
    "    \"\"\"\n",
    "    The abstract class for inference service which should implement the method.\n",
    "    \"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self,model_name, model_base_path, platform=None):\n",
    "        self.model_name = model_name\n",
    "        self.model_base_path = model_base_path\n",
    "        self.model_version_list = [1]\n",
    "        self.model_graph_signature = None\n",
    "        self.platform = platform\n",
    "\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Check args for model_platform and others\n",
    "\n",
    "# Initialize flask application\n",
    "application = Flask(__name__, template_folder='templates')\n",
    "\n",
    "\n",
    "\n",
    "UPLOAD_FOLDER = os.path.basename('static')\n",
    "application.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "\n",
    "application.secret_key='\\xf1\\x92Y\\xdf\\x8ejY\\x04\\x96\\xb4V\\x88\\xfb\\xfc\\xb5\\x18F\\xa3\\xee\\xb9\\xb9t\\x01\\xf0\\x96' #login\n",
    "model_name_service_map={}\n",
    "x={\n",
    "  \"model_config_list\": [\n",
    "    {\n",
    "      \"name\": \"tensorflow_template_application_model\",\n",
    "      \"base_path\": \"./models/tensorflow_template_application_model/\",\n",
    "      \"platform\": \"tensorflow\"\n",
    "    }, {\n",
    "      \"name\": \"deep_image_model\",\n",
    "      \"base_path\": \"./models/deep_image_model/\",\n",
    "      \"platform\": \"tensorflow\"\n",
    "    }, {\n",
    "       \"name\": \"mxnet_mlp_model\",\n",
    "       \"base_path\": \"./models/mxnet_mlp/mx_mlp\",\n",
    "       \"platform\": \"mxnet\"\n",
    "    },\n",
    "      {\n",
    "       \"name\": \"mxnet_mlp_model2\",\n",
    "       \"base_path\": \"./models/mxnet_mlp/mx_mlp\",\n",
    "       \"platform\": \"mxnet\"\n",
    "    },\n",
    "      {\n",
    "       \"name\": \"mxnet_mlp_model3\",\n",
    "       \"base_path\": \"./models/mxnet_mlp/mx_mlp\",\n",
    "       \"platform\": \"mxnet\"\n",
    "    },\n",
    "      {\n",
    "       \"name\": \"mxnet_mlp_model4\",\n",
    "       \"base_path\": \"./models/mxnet_mlp/mx_mlp\",\n",
    "       \"platform\": \"mxnet\"\n",
    "    },\n",
    "      {\n",
    "       \"name\": \"mxnet_mlp_model5\",\n",
    "       \"base_path\": \"./models/mxnet_mlp/mx_mlp\",\n",
    "       \"platform\": \"mxnet\"\n",
    "    },\n",
    "      {\n",
    "       \"name\": \"mxnet_mlp_model6\",\n",
    "       \"base_path\": \"./models/mxnet_mlp/mx_mlp\",\n",
    "       \"platform\": \"mxnet\"\n",
    "    }\n",
    "  ]\n",
    "}#{\"name\": \"tensorflow_template_application\", \"base_path\": \"/\", \"platform\": \"tensorflow\"}\n",
    "# The API to render the dashboard page\n",
    "for kk in x['model_config_list']:\n",
    "    #InferenceService\n",
    "    #print kk['name'],kk['base_path'],kk['platform']\n",
    "    model_name_service_map[kk['name']]=InferenceService(kk['name'],kk['base_path'],kk['platform'])\n",
    "    \n",
    "if args.model_config_file != \"\":\n",
    "  # Read from configuration file\n",
    "  with open(args.model_config_file) as data_file:\n",
    "    AI_server_config = yaml.load(data_file)\n",
    "    \n",
    "models_loaded = {}\n",
    "model_loader = ModelLoader(AI_server_config)\n",
    "\n",
    "@application.route(\"/login\",methods=['POST','GET'])\n",
    "def login():\n",
    "    error = None\n",
    "    if request.method == 'POST':\n",
    "        session['username']=request.form['username']\n",
    "        session['password']=request.form['password']\n",
    "        if request.form['username'] != 'admin' or request.form['password'] != 'admin123': \n",
    "                error= \"sorry\"\n",
    "        else:\n",
    "            return redirect(url_for('index'))\n",
    "    return render_template('login.html',error=error)\n",
    "test=RedisDict(namespace='web_ui')\n",
    "\n",
    "@application.route(\"/index\", methods=[\"GET\"])\n",
    "def index():\n",
    "    if session.get('username')=='admin' and session.get('password')=='admin123':\n",
    "        model_name_service_map_redis={}\n",
    "        dict_redis=test['model_info']\n",
    "        for kk in dict_redis['model_config_list']:\n",
    "            model_name_service_map_redis[kk['name']]=InferenceService(kk['name'],kk['base_path'],kk['platform'])\n",
    "        print(model_name_service_map_redis)\n",
    "        return render_template(\"index.html\", model_name_service_map=model_name_service_map_redis)\n",
    "    return \"you are not logged in\" \n",
    "\n",
    "\n",
    "# The API to rocess inference request\n",
    "@application.route(\"/\", methods=[\"POST\"])\n",
    "@login_required\n",
    "def inference():\n",
    "  # Process requests with json data\n",
    "  if request.content_type.startswith(\"application/json\"):\n",
    "    json_data = json.loads(request.data)\n",
    "\n",
    "  # Process requests with raw image\n",
    "  elif request.content_type.startswith(\"multipart/form-data\"):\n",
    "    json_data = {}\n",
    "\n",
    "    if \"model_version\" in request.form:\n",
    "      json_data[\"model_version\"] = int(request.form[\"model_version\"])\n",
    "    if \"run_profile\" in request.form:\n",
    "      json_data[\"run_profile\"] = request.form[\"run_profile\"]\n",
    "\n",
    "    image_cont1ent = request.files[\"image\"].read()\n",
    "    image_string = np.fromstring(image_content, np.uint8)\n",
    "    if sys.version_info[0] < 3:\n",
    "      import cStringIO\n",
    "      image_string_io = cStringIO.StringIO(image_string)\n",
    "    else:\n",
    "      image_string_io = io.BytesIO(image_string)\n",
    "\n",
    "    image_file = Image.open(image_string_io)\n",
    "    if \"channel_layout\" in request.form:\n",
    "      channel_layout = request.form[\"channel_layout\"]\n",
    "      if channel_layout in [\"RGB\", \"RGBA\"]:\n",
    "        image_file = image_file.convert(channel_layout)\n",
    "      else:\n",
    "        logging.error(\"Illegal image_layout: {}\".format(channel_layout))\n",
    "\n",
    "    image_array = np.array(image_file)\n",
    "\n",
    "    # TODO: Support multiple images without reshaping\n",
    "    if \"shape\" in request.form:\n",
    "      # Example: \"32,32,1,3\" -> (32, 32, 1, 3)\n",
    "      shape = tuple([int(item) for item in request.form[\"shape\"].split(\",\")])\n",
    "      image_array = image_array.reshape(shape)\n",
    "    else:\n",
    "      image_array = image_array.reshape(1, *image_array.shape)\n",
    "\n",
    "    json_data[\"data\"] = {\"image\": image_array}\n",
    "    #json_data[\"data\"] = {\"image_data\": image_array}\n",
    "\n",
    "  else:\n",
    "    logging.error(\"Unsupported content type: {}\".format(request.content_type))\n",
    "    return \"Error, unsupported content type\"\n",
    "\n",
    "  # Request backend service with json data\n",
    "  #logging.debug(\"Constructed request data as json: {}\".format(json_data))\n",
    "\n",
    "  if \"model_name\" in json_data:\n",
    "    model_name = json_data.get(\"model_name\", \"\")\n",
    "    if model_name == \"\":\n",
    "      logging.error(\"The model does not exist: {}\".format(model_name))\n",
    "  else:\n",
    "    model_name = \"default\"\n",
    "\n",
    "  inferenceService = model_name_service_map[model_name]\n",
    "  result = inferenceService.inference(json_data)\n",
    "\n",
    "  # TODO: Change the decoder for numpy data\n",
    "  return jsonify(json.loads(json.dumps(result, cls=NumpyEncoder)))\n",
    "\n",
    "\n",
    "@application.route('/image_inference', methods=[\"GET\"])\n",
    "def image_inference():\n",
    "    return render_template('image_inference.html')\n",
    "@application.route('/API_stat', methods=[\"GET\"])\n",
    "def API_stat():\n",
    "    return render_template('API_stat.html')\n",
    "\n",
    "\n",
    "@application.route('/run_image_inference', methods=['POST'])\n",
    "def run_image_inference():\n",
    "  file = request.files['image']\n",
    "  file_path = os.path.join(application.config['UPLOAD_FOLDER'], file.filename)\n",
    "  file.save(file_path)\n",
    "\n",
    "  channel_layout = \"RGB\"\n",
    "  if \"channel_layout\" in request.form:\n",
    "    channel_layout_ = request.form[\"channel_layout\"]\n",
    "    if channel_layout_ in [\"RGB\", \"RGBA\"]:\n",
    "        channel_layout = channel_layout_\n",
    "\n",
    "  run_profile = \"\"\n",
    "  if \"run_profile\" in request.form:\n",
    "    run_profile = request.form[\"run_profile\"]\n",
    "\n",
    "  image_file_path = os.path.join(application.config['UPLOAD_FOLDER'],\n",
    "                                 file.filename)\n",
    "  predict_result = python_predict_client.predict_image(\n",
    "    image_file_path, channel_layout=channel_layout, run_profile=run_profile, port=args.port)\n",
    "\n",
    "  return render_template(\n",
    "      'image_inference.html',\n",
    "      image_file_path=image_file_path,\n",
    "      predict_result=predict_result)\n",
    "\n",
    "\n",
    "@application.route('/json_inference', methods=[\"GET\"])\n",
    "def json_inference():\n",
    "  return render_template('json_inference.html')\n",
    "\n",
    "\n",
    "@application.route('/run_json_inference', methods=['POST'])\n",
    "def run_json_inference():\n",
    "  json_data_string = request.form[\"json_data\"]\n",
    "  json_data = json.loads(json_data_string)\n",
    "  model_name = request.form[\"model_name\"]\n",
    "\n",
    "  request_json_data = {\"model_name\": model_name, \"data\": json_data}\n",
    "\n",
    "  predict_result = python_predict_client.predict_json(request_json_data, port=args.port)\n",
    "\n",
    "  return render_template('json_inference.html', predict_result=predict_result)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Start the HTTP server\n",
    "    # Support multi-thread for json inference and image inference in same process\n",
    "    application.run(host='127.0.0.1', port=8024, threaded=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "\n",
    "class InferenceService(object):\n",
    "    \"\"\"\n",
    "    The abstract class for inference service which should implement the method.\n",
    "    \"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self,model_name, model_base_path, platform=None):\n",
    "        self.model_name = model_name\n",
    "        self.model_base_path = model_base_path\n",
    "        self.model_version_list = [1]\n",
    "        self.model_graph_signature = None\n",
    "        self.platform = platform\n",
    "\n",
    "    ##@abstractmethod\n",
    "    #def inference(self, json_data):\n",
    "    #    \"\"\"\n",
    "    #    Args:\n",
    "    #      json_data: The JSON serialized object with key and array data.\n",
    "    #    Return:\n",
    "    #      The JSON serialized object with key and array data.\n",
    "    #    \"\"\"\n",
    "     #   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow_template_application_model ./models/tensorflow_template_application_model/ tensorflow\n",
      "{'platform': 'tensorflow', 'name': 'tensorflow_template_application_model', 'base_path': './models/tensorflow_template_application_model/'}\n",
      "deep_image_model ./models/deep_image_model/ tensorflow\n",
      "{'platform': 'tensorflow', 'name': 'deep_image_model', 'base_path': './models/deep_image_model/'}\n",
      "mxnet_mlp_model ./models/mxnet_mlp/mx_mlp mxnet\n",
      "{'platform': 'mxnet', 'name': 'mxnet_mlp_model', 'base_path': './models/mxnet_mlp/mx_mlp'}\n"
     ]
    }
   ],
   "source": [
    "model_name_service_map={}\n",
    "x={\n",
    "  \"model_config_list\": [\n",
    "    {\n",
    "      \"name\": \"tensorflow_template_application_model\",\n",
    "      \"base_path\": \"./models/tensorflow_template_application_model/\",\n",
    "      \"platform\": \"tensorflow\"\n",
    "    }, {\n",
    "      \"name\": \"deep_image_model\",\n",
    "      \"base_path\": \"./models/deep_image_model/\",\n",
    "      \"platform\": \"tensorflow\"\n",
    "    }, {\n",
    "       \"name\": \"mxnet_mlp_model\",\n",
    "       \"base_path\": \"./models/mxnet_mlp/mx_mlp\",\n",
    "       \"platform\": \"mxnet\"\n",
    "    }\n",
    "  ]\n",
    "}#{\"name\": \"tensorflow_template_application\", \"base_path\": \"/\", \"platform\": \"tensorflow\"}\n",
    "# The API to render the dashboard page\n",
    "for kk in x['model_config_list']:\n",
    "    #InferenceService\n",
    "    print kk['name'],kk['base_path'],kk['platform']\n",
    "    model_name_service_map[kk['name']]=InferenceService(kk['name'],kk['base_path'],kk['platform'])\n",
    "    print kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep_image_model': <__main__.InferenceService at 0x10baab290>,\n",
       " 'mxnet_mlp_model': <__main__.InferenceService at 0x10baed0d0>,\n",
       " 'tensorflow_template_application_model': <__main__.InferenceService at 0x10baab1d0>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_service_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis_test import RedisDict\n",
    "test=RedisDict(namespace='web_ui')\n",
    "model_name_service_map_redis=test['model_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_name_service_map_redis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
