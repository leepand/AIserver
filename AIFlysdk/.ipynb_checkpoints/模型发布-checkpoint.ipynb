{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from cloudpickle import CloudPickler\n",
    "#from ..clipper_admin import CLIPPER_TEMP_DIR\n",
    "CLIPPER_TEMP_DIR='./'\n",
    "import os\n",
    "import tempfile\n",
    "import sys\n",
    "#cur_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "if sys.version_info < (3, 0):\n",
    "    try:\n",
    "        from cStringIO import StringIO\n",
    "    except ImportError:\n",
    "        from StringIO import StringIO\n",
    "    PY3 = False\n",
    "else:\n",
    "    from io import BytesIO as StringIO\n",
    "    PY3 = True\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def serialize_object(obj):\n",
    "    s = StringIO()\n",
    "    c = CloudPickler(s, 2)\n",
    "    c.dump(obj)\n",
    "    return s.getvalue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x80\\x02U\\x01dq\\x00.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialize_object('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_python_function(name, func):\n",
    "    predict_fname = \"func.pkl\"\n",
    "\n",
    "    # Serialize function\n",
    "    s = StringIO()\n",
    "    c = CloudPickler(s, 2)\n",
    "    c.dump(func)\n",
    "    serialized_prediction_function = s.getvalue()\n",
    "\n",
    "    # Set up serialization directory\n",
    "    if not os.path.exists(CLIPPER_TEMP_DIR):\n",
    "        os.makedirs(CLIPPER_TEMP_DIR)\n",
    "    serialization_dir = tempfile.mkdtemp(dir=CLIPPER_TEMP_DIR)\n",
    "    logger.info(\"Saving function to {}\".format(serialization_dir))\n",
    "\n",
    "    # Write out function serialization\n",
    "    func_file_path = os.path.join(serialization_dir, predict_fname)\n",
    "    if sys.version_info < (3, 0):\n",
    "        with open(func_file_path, \"w\") as serialized_function_file:\n",
    "            serialized_function_file.write(serialized_prediction_function)\n",
    "    else:\n",
    "        with open(func_file_path, \"wb\") as serialized_function_file:\n",
    "            serialized_function_file.write(serialized_prediction_function)\n",
    "    logging.info(\"Serialized and supplied predict function\")\n",
    "    return serialization_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./tmpmr8kOE'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def center(xs):\n",
    "    means = np.mean(xs, axis=0)\n",
    "    return xs - means\n",
    "save_python_function('fib',center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing storage_clients/local_fs/storage_client.py\n"
     ]
    }
   ],
   "source": [
    "%%file storage_clients/local_fs/storage_client.py\n",
    "\n",
    "import os\n",
    "\n",
    "class StorageClient(object):\n",
    "    def __init__(self, storage_client_config):\n",
    "        self.__modelrepo_dir = storage_client_config[\"modelrepo_dir\"]\n",
    "        self.__model_repo_local_path = self.__modelrepo_dir + '/{model_id}_{model_version}'\n",
    "\n",
    "    def ___model_repo_local_path(self, model_id, model_version):\n",
    "        \"\"\"\n",
    "        Returns the local path of model Repository for the given model id and model version.\n",
    "        Args:\n",
    "            model_id:\n",
    "            model_version:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        return self.__model_repo_local_path.format(model_id=model_id, model_version=model_version.replace('.', '_'))\n",
    "\n",
    "    def get_model_blob(self, model_id, model_version):\n",
    "        \"\"\"\n",
    "        Returns model blob for the given model id and model version\n",
    "        Args:\n",
    "            model_id:\n",
    "            model_version:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        model_file_path = self.___model_repo_local_path(model_id=model_id, model_version=model_version)\n",
    "        if os.path.exists(model_file_path):\n",
    "            with open(model_file_path, \"r\") as f:\n",
    "                model_blob = f.read()\n",
    "                return model_blob\n",
    "        else:\n",
    "            raise Exception(\"Model ({}, {}) doesn't exist. \".format(model_id, model_version))\n",
    "\n",
    "    def write_model_blob(self, model_blob, model_id, model_version):\n",
    "        \"\"\"\n",
    "        Write model blob with the given model id and model version to Model Repository storage.\n",
    "        Args:\n",
    "            model_blob:\n",
    "            model_id:\n",
    "            model_version:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        model_file_path = self.___model_repo_local_path(model_id=model_id, model_version=model_version)\n",
    "        if os.path.exists(model_file_path):\n",
    "            raise Exception(\"Model ({}, {}) already exists. \".format(model_id, model_version))\n",
    "        else:\n",
    "            with open(model_file_path, \"wb\") as f:\n",
    "                f.write(model_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting storage_clients/storage_client.py\n"
     ]
    }
   ],
   "source": [
    "%%file storage_clients/storage_client.py\n",
    "import importlib\n",
    "\n",
    "def get_storage_client(model_storage_backend='local_fs', model_storage_client_config=None):\n",
    "    \"\"\"\n",
    "    Returns the storage client instance created with given storage backend and storage config\n",
    "    Args:\n",
    "        model_storage_backend: Blob storage backend to be used. For ex: local_fs,s3, azure_blob_storage etc.\n",
    "        model_storage_client_config: Configuration required to connect to and load from Blob Storage. For ex: S3 Configuration parameters like endpoint, bucket, secret key, access key etc.\n",
    "    Returns: storage client instance which allows to write model blobs and load model blobs.\n",
    "    \"\"\"\n",
    "    package = \"AIFlysdk.storage_clients.\" + model_storage_backend#AIFlysdk.\n",
    "    storage_client_module = importlib.import_module(\".storage_client\", package)\n",
    "    StorageClient = getattr(storage_client_module, 'StorageClient')\n",
    "    return StorageClient(model_storage_client_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing storage_clients/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%file storage_clients/__init__.py\n",
    "__version__=0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing storage_clients/local_fs/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%file storage_clients/local_fs/__init__.py\n",
    "__version__=0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting AIFly_publisher.py\n"
     ]
    }
   ],
   "source": [
    "%%file AIFly_publisher.py\n",
    "import platform\n",
    "import os\n",
    "import importlib\n",
    "import inspect\n",
    "import os\n",
    "import platform\n",
    "import stat\n",
    "import sys\n",
    "from io import BytesIO\n",
    "from tarfile import TarFile\n",
    "import cloudpickle\n",
    "from pkg_resources import find_distributions, DistributionNotFound\n",
    "from setuptools import sandbox\n",
    "from AIFlysdk.storage_clients.storage_client import get_storage_client\n",
    "\n",
    "class Publisher:\n",
    "    def __init__(self, AIaasFly_client_config):\n",
    "        self.__model_storage_backend = AIaasFly_client_config[\"model_storage\"][\"backend\"]\n",
    "        self.__model_storage_client_config = AIaasFly_client_config[\"model_storage\"][self.__model_storage_backend]\n",
    "\n",
    "    def write_model_to_blob_storage(self, model_blob, model_id, model_version):\n",
    "        storage_client = get_storage_client(self.__model_storage_backend, self.__model_storage_client_config)\n",
    "        storage_client.write_model_blob(model_blob, model_id, model_version)\n",
    "    def publish_model(self, model_instance, model_id, model_version, path_to_setup_py = None, custom_package_name = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_instance:\n",
    "            path_to_setup_py:\n",
    "            custom_package_name:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        model_blob, model_meta = self._create_model_blob(None, None, path_to_setup_py, custom_package_name, cloudpickle.dumps(model_instance))\n",
    "        self.write_model_to_blob_storage(cloudpickle.dumps(model_blob), model_id, model_version)\n",
    "\n",
    "\n",
    "    def publish_asm_model(self, path_to_prediction_module, path_to_model_resources_dir, model_id, model_version, path_to_setup_py=None, custom_package_name=None):\n",
    "        self._prediction_module_guardrails_breach(path_to_prediction_module, path_to_model_resources_dir, path_to_setup_py)\n",
    "        model_blob, model_meta = self._create_model_blob(path_to_prediction_module, path_to_model_resources_dir, path_to_setup_py, custom_package_name)\n",
    "        self.write_model_to_blob_storage(cloudpickle.dumps(model_blob), model_id, model_version)\n",
    "    def _prediction_module_guardrails_breach(self, prediciton_module_file_path, saved_model_path, path_to_setup_py):\n",
    "        \"\"\"\n",
    "        This method checks if the prediction module, breaches any of the following guardrails.\n",
    "        1. Prediction module implements a load_model function outside of the Model class, to instantiate and load the model.\n",
    "        2. load_model should just take one and only argument, named model_dir.\n",
    "        3. model_dir should be a directory, not a file.\n",
    "        4. load_model should be able to load the model using the saved_model_path.\n",
    "        5. predict method should be implemented in the model class.\n",
    "        :param saved_model_path:\n",
    "        :param prediciton_module_file_path\n",
    "        :return: False, if there are no breaches. Raises exception otherwise.\n",
    "        \"\"\"\n",
    "        if path_to_setup_py:\n",
    "            setup_py_directory = None\n",
    "            if os.path.isfile(path_to_setup_py):\n",
    "                path_to_setup_py = os.path.abspath(path_to_setup_py)\n",
    "                setup_py_directory = os.path.dirname(path_to_setup_py)\n",
    "            elif os.path.isdir(path_to_setup_py):\n",
    "                setup_py_directory = os.path.abspath(path_to_setup_py)\n",
    "                path_to_setup_py = setup_py_directory + '/setup.py'\n",
    "            sys.path.append(setup_py_directory)\n",
    "\n",
    "        basepath, module_name = os.path.split(prediciton_module_file_path)\n",
    "        extension = module_name[-3:]\n",
    "        if extension != '.py':\n",
    "            raise TypeError('Please provide the full path to python hunch prediction file')\n",
    "        module_name = module_name[:-3]\n",
    "        sys.path.insert(0, basepath)\n",
    "        prediction_module = importlib.import_module(module_name)\n",
    "        load_model = getattr(prediction_module, 'load_model', None)\n",
    "        if not load_model:\n",
    "            raise NotImplementedError('Load method not implemented in the prediction module.')\n",
    "\n",
    "        arguments = None\n",
    "        try:\n",
    "            arguments = inspect.getargspec(load_model).args\n",
    "        except TypeError:\n",
    "            raise NotImplementedError(\"Implement Load function correctly. Refer to docs.\")\n",
    "\n",
    "        if len(arguments) != 1:\n",
    "            raise AttributeError('load_model method should have one and only one argument: model_dir')\n",
    "\n",
    "        if not os.path.isdir(saved_model_path):\n",
    "            raise RuntimeError('{!r} is not a directory'.format(saved_model_path))\n",
    "\n",
    "        try:\n",
    "            model_instance = load_model(saved_model_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('Local load not possible with the model_dir that was provided and load_model that was implemented')\n",
    "\n",
    "        if not callable(getattr(model_instance, 'predict')):\n",
    "            raise NotImplementedError('predict not implemented in the model class.')\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _create_model_blob(self, path_to_prediction_module,path_to_model_resources_dir, path_to_setup_py=None, custom_package_name=None, model_dump=None):\n",
    "        \"\"\"\n",
    "        Creates the blob to be stored for the alternate serialization model.\n",
    "        :param self:\n",
    "        :param path_to_prediction_module:\n",
    "        :param path_to_model_resources_dir:\n",
    "        :param path_to_setup_py:\n",
    "        :param custom_package_name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        model_blob = {}\n",
    "        model_meta_data = {}\n",
    "        if path_to_setup_py is not None:\n",
    "            package_info = self.create_package_from_setup_py(path_to_setup_py, custom_package_name)\n",
    "            custom_package_blob = self._create_model_blob_details_with_custom_setup(None, package_info['name'],\n",
    "                                                                                    package_info['version'],\n",
    "                                                                                    package_info['path'])\n",
    "            model_blob.update(custom_package_blob)\n",
    "\n",
    "        if path_to_prediction_module is not None:\n",
    "            prediction_blob = self._get_prediction_module_tar_byte_buffer(path_to_prediction_module)\n",
    "            resources_blob = self._get_model_resources_tar_byte_buffer(path_to_model_resources_dir)\n",
    "            model_blob['model_predict_module'] = prediction_blob.getvalue()\n",
    "            model_blob['modeldir_blob'] = resources_blob.getvalue()\n",
    "            model_blob['serialization_mechanism'] = 'asm'\n",
    "        else:\n",
    "            model_blob['model_blob'] = model_dump\n",
    "\n",
    "        model_blob['platform_details'] = {\n",
    "            'architecture': platform.machine(),\n",
    "            'os_name': platform.system(),\n",
    "            'processor': platform.processor(),\n",
    "            'os_version': platform.dist()\n",
    "        }\n",
    "        model_blob['python_details'] = {\n",
    "            'python_version': platform.python_version(),\n",
    "            'python_compiler': platform.python_compiler(),\n",
    "            'python_implementation': platform.python_implementation()\n",
    "        }\n",
    "        model_blob['gcc_details'] = {\n",
    "            'gcc_version': platform.python_compiler().split(' ')[1],\n",
    "            'glibc_version': platform.libc_ver()[0]\n",
    "        }\n",
    "        model_blob['specification_version'] = '1.0'\n",
    "        model_meta_data['python_version'] = model_blob['python_details']['python_version']\n",
    "        model_meta_data['gcc_version'] = model_blob['gcc_details']['gcc_version']\n",
    "        model_meta_data['os_name'] = model_blob['platform_details']['os_name']\n",
    "        model_meta_data['os_flavour'] = model_blob['platform_details']['os_version'][0]\n",
    "        model_meta_data['os_version'] = model_blob['platform_details']['os_version'][1]\n",
    "        return model_blob, model_meta_data\n",
    "    def _get_prediction_module_tar_byte_buffer(self, path_to_prediction_module):\n",
    "        \"\"\"\n",
    "        Returns a Byte buffer of a tar file containing the prediction module. The tar file is compressed using bz2\n",
    "        :param path_to_prediction_module: Path to prediction module file\n",
    "        :return: Byte buffer with the tar data\n",
    "        \"\"\"\n",
    "        prediction_module_stat = os.stat(path_to_prediction_module)\n",
    "        if stat.S_ISDIR(prediction_module_stat.st_mode):\n",
    "            raise Exception(\"Expected a file but got a directory for arg 'path_to_prediction_module' = '{}'\".format(\n",
    "                path_to_prediction_module))\n",
    "\n",
    "        file_out = BytesIO()\n",
    "        with TarFile.open(mode=\"w:bz2\", fileobj=file_out) as tar:\n",
    "            tar.add(name=path_to_prediction_module, arcname='model.py')\n",
    "\n",
    "        return file_out\n",
    "    def _get_model_resources_tar_byte_buffer(self, path_to_model_resources_dir):\n",
    "        \"\"\"\n",
    "        Returns a byte buffer of a tar file containing the model resources. The tar file is compressed using bz2.\n",
    "        The topmost folder is named 'model_resource'\n",
    "        :param path_to_model_resources_dir:  Path to resources directory\n",
    "        :return: Byte buffer with the tar data\n",
    "        \"\"\"\n",
    "        model_resources_stat = os.stat(path_to_model_resources_dir)\n",
    "        if not stat.S_ISDIR(model_resources_stat.st_mode):\n",
    "            raise Exception(\n",
    "                \"Expected a directory for arg 'path_to_model_resources_dir' = {}\".format(path_to_model_resources_dir))\n",
    "\n",
    "        file_out = BytesIO()\n",
    "        with TarFile.open(mode=\"w:bz2\", fileobj=file_out) as tar:\n",
    "            tar.add(name=path_to_model_resources_dir, recursive=True, arcname='')\n",
    "\n",
    "        return file_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function platform.python_compiler>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.machine()\n",
    "platform.python_compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AIFlyApi.py\n"
     ]
    }
   ],
   "source": [
    "%%file AIFlyApi.py\n",
    "import os\n",
    "import yaml\n",
    "from AIFly_publisher import Publisher\n",
    "\n",
    "class AIFlyApi:\n",
    "\n",
    "    def __init__(self, config=\"client_config.yaml\"):\n",
    "        config_file = config\n",
    "        env_value = os.getenv(\"AIFly_API_CONFIG\", None)\n",
    "        if env_value:\n",
    "            config_file = env_value\n",
    "\n",
    "        if os.path.exists(config_file):\n",
    "            with open(config_file, 'rt') as f:\n",
    "                client_config = yaml.load(f.read())\n",
    "                self.publisher = Publisher(client_config)\n",
    "        else:\n",
    "            raise Exception(\"Given config file doesn't exist\")\n",
    "    \n",
    "    def publish_model(self, model_instance, model_id, model_version, path_to_setup_py = None, custom_package_name = None):\n",
    "        \"\"\"\n",
    "        Publishes the given model to Model Blob Storage. It uses Cloudpickle (https://github.com/cloudpipe/cloudpickle) based serialization.\n",
    "        Args:\n",
    "            model_instance: Model object which has predict method implemented\n",
    "            model_id: Model ID\n",
    "            model_version: Model Version. Model Version is not auto generated. Model Repository integration is needed for Auto version increment.\n",
    "            path_to_setup_py: Path to setup.py of the custom package on which model is dependent\n",
    "            custom_package_name: Name of the custom package on which the model is dependent.\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        self.publisher.publish_model(model_instance, model_id, model_version, path_to_setup_py, custom_package_name)\n",
    "    def publish_asm_model(self, path_to_prediction_module, path_to_model_resources_dir, model_id, model_version, path_to_setup_py=None, custom_package_name=None):\n",
    "        \"\"\"\n",
    "        Publishes the given model to Model Blob Storage. Models which are not pure Python objects ( Eg: Models built using frameworks like tensorflow, fasttext, crf etc.) are not serializable using Cloudpickle.\n",
    "        Args:\n",
    "            path_to_prediction_module: Prediction module is where you write the model class and implement load_model\n",
    "            path_to_model_resources_dir: Model resources directory. All the files needed to load a model should be there in this directory.\n",
    "            model_id: Model ID\n",
    "            model_version: Model Version. Model Version is not auto generated. Model Repository integration is needed for Auto version increment.\n",
    "            path_to_setup_py: Path to setup.py of the custom package on which model is dependent\n",
    "            custom_package_name: Name of the custom package on which the model is dependent.\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        self.publisher.publish_asm_model(path_to_prediction_module, path_to_model_resources_dir, model_id, model_version, path_to_setup_py, custom_package_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#y=AIFlyApi(config='config/AIFlyapi_config.yaml')\n",
    "#y.publish_asm_model('model/fib.py','./model_resource','_test','0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Prediction: \"Hello World\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "class HelloWorldModel:\n",
    "    def predict(self, input):\n",
    "        return json.dumps(\"Hello World\")\n",
    "\n",
    "model = HelloWorldModel()\n",
    "print \"Local Prediction:\", model.predict(None)\n",
    "y.publish_model(model, \"HelloWorldExample\", \"1.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Prediction: \"I am ai server\"\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Model (HelloAI2, 1.0.0) already exists. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8e29b2ad9b6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHelloWorldModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Local Prediction:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HelloAI2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1.0.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-146e833ccf29>\u001b[0m in \u001b[0;36mpublish_model\u001b[0;34m(self, model_instance, model_id, model_version, path_to_setup_py, custom_package_name)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \"\"\"\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublisher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_setup_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_package_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_asm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_prediction_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_model_resources_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_setup_py\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_package_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n",
      "\u001b[0;32m/Users/leepand/Downloads/BigRLab_APIs/demoday_fs/web_lab/AIaas/AIserver/AIaasFly/AIFlysdk/AIFly_publisher.py\u001b[0m in \u001b[0;36mpublish_model\u001b[0;34m(self, model_instance, model_id, model_version, path_to_setup_py, custom_package_name)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \"\"\"\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel_blob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model_blob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_setup_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_package_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_model_to_blob_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leepand/Downloads/BigRLab_APIs/demoday_fs/web_lab/AIaas/AIserver/AIaasFly/AIFlysdk/AIFly_publisher.py\u001b[0m in \u001b[0;36mwrite_model_to_blob_storage\u001b[0;34m(self, model_blob, model_id, model_version)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_model_to_blob_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_blob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstorage_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_storage_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__model_storage_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__model_storage_client_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mstorage_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_model_blob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_blob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_setup_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_package_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \"\"\"\n",
      "\u001b[0;32m/Users/leepand/Downloads/BigRLab_APIs/demoday_fs/web_lab/AIaas/AIserver/AIaasFly/AIFlysdk/storage_clients/local_fs/storage_client.pyc\u001b[0m in \u001b[0;36mwrite_model_blob\u001b[0;34m(self, model_blob, model_id, model_version)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___model_repo_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model ({}, {}) already exists. \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Model (HelloAI2, 1.0.0) already exists. "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from AIFlysdk.AIFlyApi import AIFlyApi\n",
    "y=AIFlyApi(config='config/AIFlyapi_config.yaml')\n",
    "class HelloWorldModel:\n",
    "    def predict(self, input):\n",
    "        return json.dumps(\"I am ai server\")\n",
    "\n",
    "model = HelloWorldModel()\n",
    "print \"Local Prediction:\", model.predict(None)\n",
    "y.publish_model(model, \"HelloAI2\", \"1.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leepand/Downloads/BigRLab_APIs/demoday_fs/web_lab/AIaas/AIserver/AIaasFly/AIFlysdk/model_resource/resources/prediction_modules/tf/mnist_canned_estimator/iris_estimator_predict.py\n"
     ]
    }
   ],
   "source": [
    "print dir_path+'/model_resource/resources/prediction_modules/tf/mnist_canned_estimator/iris_estimator_predict.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leepand/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x119348d90>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/Users/leepand/Downloads/BigRLab_APIs/demoday_fs/web_lab/AIaas/AIserver/AIaasFly/AIFlysdk/model_resource/model_resources/tf/mnist_canned_estimator', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir_path='/Users/leepand/Downloads/BigRLab_APIs/demoday_fs/web_lab/AIaas/AIserver/AIaasFly/AIFlysdk'\n",
    "y.publish_asm_model(path_to_prediction_module=dir_path+'/model_resource/prediction_modules/tf/mnist_canned_estimator/iris_estimator_predict.py',\n",
    "            path_to_model_resources_dir=dir_path+'/model_resource/model_resources/tf/mnist_canned_estimator',\n",
    "            model_id=\"TensorflowMnistExample\",\n",
    "                    model_version=\"1.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_dir):\n",
    "    return fib(model_dir)\n",
    "\n",
    "\n",
    "class fib:\n",
    "    def __init__(self,model_dir):\n",
    "        self.model_dir=None\n",
    "\n",
    "    def predict(self, n):\n",
    "        if n == 0 or n == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return self.predict(n-1) + self.predict(n-2)\n",
    "\n",
    "load_model('d').predict(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "y=AIFlyApi(config='config/AIFlyapi_config.yaml')\n",
    "dir_path='/Users/leepand/Downloads/BigRLab_APIs/demoday_fs/web_lab/AIaas/AIserver/AIaasFly/AIFlysdk'\n",
    "y.publish_asm_model(path_to_prediction_module=dir_path+'/model/pred/fib.py',\n",
    "            path_to_model_resources_dir=dir_path+'/model/model',\n",
    "            model_id=\"fib_model\",\n",
    "                    model_version=\"1.0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named get_storage_client",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d7a8f221bab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstorage_clients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_storage_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named get_storage_client"
     ]
    }
   ],
   "source": [
    "from storage_clients.storage_client.get_storage_client import get_storage_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config/AIFlyapi_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%file  config/AIFlyapi_config.yaml\n",
    "model_storage:\n",
    "  backend: \"local_fs\"\n",
    "  local_fs:\n",
    "    modelrepo_dir: \"./tmp/model_repo\"\n",
    "  s3:\n",
    "    access_key: \"Dummy Access Key\"\n",
    "    bucket: \"Dummy S3 bucket\"\n",
    "    chunk_size: 134217728\n",
    "    endpoint: \"S3 Endpoint\"\n",
    "    max_error_retry: 5\n",
    "    secret_key: \"Dummy Scret Key\"\n",
    "    size_limit: 209715100\n",
    "model_loader:\n",
    "    working_dir: \"/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from tarfile import TarFile\n",
    "def _get_model_resources_tar_byte_buffer( path_to_model_resources_dir):\n",
    "        \"\"\"\n",
    "        Returns a byte buffer of a tar file containing the model resources. The tar file is compressed using bz2.\n",
    "        The topmost folder is named 'model_resource'\n",
    "        :param path_to_model_resources_dir:  Path to resources directory\n",
    "        :return: Byte buffer with the tar data\n",
    "        \"\"\"\n",
    "        model_resources_stat = os.stat(path_to_model_resources_dir)\n",
    "        if not stat.S_ISDIR(model_resources_stat.st_mode):\n",
    "            raise Exception(\n",
    "                \"Expected a directory for arg 'path_to_model_resources_dir' = {}\".format(path_to_model_resources_dir))\n",
    "\n",
    "        file_out = BytesIO()\n",
    "        with TarFile.open(mode=\"w:bz2\", fileobj=file_out) as tar:\n",
    "            tar.add(name=path_to_model_resources_dir, recursive=True, arcname='')\n",
    "\n",
    "        return file_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_to_blob_storage(model_blob, model_id, model_version):\n",
    "        storage_client = get_storage_client(self.__model_storage_backend, self.__model_storage_client_config)\n",
    "        storage_client.write_model_blob(model_blob, model_id, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=_get_model_resources_tar_byte_buffer('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_blob={}\n",
    "model_blob['modeldir_blob']=x.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "#cloudpickle.dumps(model_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_model_to_blob_storage(cloudpickle.dumps(model_blob), model_id, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_blob( model_blob, model_id, model_version):\n",
    "        \"\"\"\n",
    "        Write model blob with the given model id and model version to Model Repository storage.\n",
    "        Args:\n",
    "            model_blob:\n",
    "            model_id:\n",
    "            model_version:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        model_file_path = './tmp/model'#self.___model_repo_local_path(model_id=model_id, model_version=model_version)\n",
    "        if os.path.exists(model_file_path):\n",
    "            raise Exception(\"Model ({}, {}) already exists. \".format(model_id, model_version))\n",
    "        else:\n",
    "            with open(model_file_path, \"wb\") as f:\n",
    "                f.write(model_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_model_blob(cloudpickle.dumps(model_blob),'fib','v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 1 must be string or buffer, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-33f5d081e25b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_model_blob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_blob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fib'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'v1.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-99ba38ca7e8a>\u001b[0m in \u001b[0;36mwrite_model_blob\u001b[0;34m(model_blob, model_id, model_version)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: argument 1 must be string or buffer, not dict"
     ]
    }
   ],
   "source": [
    "write_model_blob(model_blob,'fib','v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' does not have the buffer interface",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-dfb448c235d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_obj2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'modeldir_blob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_resources_tar_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_obj2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'modeldir_blob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mTarFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_resources_tar_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r:bz2'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_resource_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' does not have the buffer interface"
     ]
    }
   ],
   "source": [
    "model_obj2={}\n",
    "model_obj2['modeldir_blob']={}\n",
    "model_resources_tar_contents = model_obj2['modeldir_blob']\n",
    "with TarFile.open(fileobj=BytesIO(model_resources_tar_contents), mode='r:bz2') as tar:\n",
    "    tar.extractall(model_resource_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path='./tmp/model'\n",
    "if os.path.exists(model_file_path):\n",
    "    with open(model_file_path, \"r\") as f:\n",
    "        model_blob2 = f.read()\n",
    "        #return model_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_blob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8bd073953c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_blob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_blob' is not defined"
     ]
    }
   ],
   "source": [
    "model_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = cloudpickle.loads(model_blob2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modeldir_blob': \"BZh91AY&SYL\\x8d\\x9d'\\x00\\x00\\x82\\xfb\\x90\\xec\\xb0\\x02\\x80H\\xeb\\xff\\x92\\x00\\x1a\\xff7\\xde \\x04 \\x00\\x080\\x00\\xda\\xb6\\tP\\x8c\\x93\\xc50\\x8d4\\x00\\x0044\\xd1\\xa6\\xd4\\x1a\\x9a\\x11=&\\x83\\x04\\xd3M\\xa9\\x89\\x89\\x80&\\x98\\x8c%\\x14\\x8d\\xa6\\x8d@\\x00\\xc9\\x9a\\x81\\xea\\x19\\x00\\r r>f\\xa3CKQ\\x00\\xdf(\\x00\\x8f\\xa2\\xa6\\xa4\\x84DF\\x17\\xcc\\r\\x0c\\xe2\\x88\\x80\\xa0)\\nM\\xc3\\xd3\\xa6\\xc9\\x03\\x01\\xf0\\x8e.+J\\x94T\\x9d\\xb7E$\\xb2\\xc7\\xd2\\xb4s\\x9d\\xd3$\\xf3\\x1d\\xb7f\\x04U\\x01+\\xb3\\x10\\x9c\\x05\\x0b\\x07I]R\\x8c \\xc62\\x05\\xc6\\x8a?\\x19K\\x9c\\x085*B\\x0c\\\\\\x8e\\x14\\xa2Q\\xc2\\x02L@\\xe1/-c\\x88\\xd9H\\xb4\\xe6\\xf06\\x043\\x024\\xa4\\xd6\\xff\\xae\\xda\\xa2\\xbeK#8\\nm\\xa5\\x08\\x85\\x8c\\x06\\xb5\\xebd\\xf4\\x0fy|\\xac\\x13j!8\\xf5\\xb9\\xa4\\x0b\\x86\\x0e\\\\\\xc3H\\xaf\\xa74/\\x18P\\xc9\\xc2b\\xad\\x04\\t\\xaa\\x10\\x80\\xfe.\\xe4\\x8ap\\xa1 \\x99\\x1b:N\"}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_name='fib.py'\n",
    "import importlib\n",
    "module_name = module_name[:-3]\n",
    "prediction_module = importlib.import_module(module_name)\n",
    "load_model = getattr(prediction_module, 'load_model', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print getattr(prediction_module, 'load_model', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "None is not a Python function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-02c704324170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/leepand/anaconda2/lib/python2.7/inspect.pyc\u001b[0m in \u001b[0;36mgetargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is not a Python function'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mArgSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: None is not a Python function"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "arguments = inspect.getargspec(load_model).args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from AIFlysdk.AIFlyApi import AIFlyApi\n",
    "y=AIFlyApi(config='config/AIFlyapi_config.yaml')\n",
    "dir_path='/root/AIserver'\n",
    "y.publish_asm_model(path_to_prediction_module=dir_path+'/model/pred/fib.py',\n",
    "            path_to_model_resources_dir=dir_path+'/model/model',\n",
    "            model_id=\"fib_model\",\n",
    "                    model_version=\"1.0.2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
