{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting AlgoResource_new.py\n"
     ]
    }
   ],
   "source": [
    "%%file AlgoResource_new.py\n",
    "# -*- coding:utf-8 -*-\n",
    "import falcon\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from AIFlysdk.AIFlyApi import AIFlyApi\n",
    "from model_loader import ModelLoader\n",
    "import yaml\n",
    "\n",
    "try:\n",
    "    from inspect import signature, _empty\n",
    "except:\n",
    "    from funcsigs import signature, _empty\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('Algo Engine')\n",
    "PY2 = (sys.version_info.major == 2)\n",
    "PY3 = (sys.version_info.major == 3)\n",
    "\n",
    "class PredictionInput:\n",
    "    \"\"\"\n",
    "    Contains input details and model_id and data\n",
    "    model_id:'tensormodel'\n",
    "    model_version:'0.1.0\n",
    "    data:\n",
    "        - input:数据输入\n",
    "    \"\"\"  \n",
    "    def __init__(self,model_id,model_version,data):\n",
    "        self.model_id=model_id\n",
    "        self.model_version=model_version\n",
    "        self.data=data\n",
    "\n",
    "class ModelInput:\n",
    "    \"\"\"\n",
    "    Contains input details and model_id and data\n",
    "    funcslist:['predpy.pred']\n",
    "        - predpy:文件名\n",
    "        - pred:函数名\n",
    "    \"\"\"  \n",
    "    def __init__(self,funcslist):\n",
    "        self.funcslist=funcslist\n",
    "class AlgoResource(object):\n",
    "    def __init__(self,config):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.conf=config#'config/AIFlyapi_config.yaml'\n",
    "        self.models_loaded={}\n",
    "        with open(self.conf, 'rt') as f:\n",
    "            self.client_config = yaml.load(f.read())\n",
    "        #model_list=[['fib_model','1.0.2']]\n",
    "        #model_loader = ModelLoader(self.client_config)\n",
    "        #models_to_load=model_list\n",
    "        self.models_loaded = {}#model_loader.get_models_from_list(models_to_load)\n",
    "\n",
    "    def on_get(self, req, resp):\n",
    "        \"\"\"Handles GET requests\"\"\"\n",
    "        resp.status = falcon.HTTP_200  # This is the default status\n",
    "        resp.body = ('\\nFalcon is awesome! \\n')\n",
    "    def on_put(self, req, resp):\n",
    "        \"\"\"Handles PUT requests for model init\"\"\"\n",
    "        funcs_to_service_map = json.loads(req.stream.read())\n",
    "        self.logger.debug('Update request received with payload: %s', str(funcs_to_service_map))\n",
    "        model_list=funcs_to_service_map['model_list']\n",
    "        model_loader = ModelLoader(self.client_config)\n",
    "        models_to_load=model_list\n",
    "        self.models_loaded = model_loader.get_models_from_list(models_to_load)\n",
    "        #model_list=[[\"HelloWorldExample\", \"1.0.0\"],[\"HelloAI2\", \"1.0.0\"],[\"TensorflowMnistExample\",\"1.0.0\"],['fib_model',\"1.0.2\"]]\n",
    "        \n",
    "        resp.status = falcon.HTTP_200\n",
    "        #resp.content=json.dumps({\"reg info\":\"sucess\"})\n",
    "        resp.body = json.dumps({\"Init Model Service\": \"Success\"})\n",
    "    def on_post(self, req, resp):\n",
    "        payload = json.loads(req.stream.read())\n",
    "\n",
    "        if ('data' in payload.keys()) and ('modelId' in payload.keys()):\n",
    "            PredictionInput.data = payload['data']\n",
    "            PredictionInput.model_id = payload['modelId']\n",
    "            PredictionInput.model_version=payload['model_version']\n",
    "            key = (PredictionInput.model_id, PredictionInput.model_version)\n",
    "            curr_model = self.models_loaded[key]\n",
    "             \n",
    "        else:\n",
    "            resp.status = falcon.HTTP_400\n",
    "            raise falcon.HTTPBadRequest(\"Bad Request\", \"Url and(or) modelId missing in the payload\")\n",
    "\n",
    "        #result=algo_engine.call_func(fun_path,**{'n':42})\n",
    "        try:\n",
    "            po = json.dumps({'result':curr_model.predict(PredictionInput.data)})\n",
    "            resp.status = falcon.HTTP_200\n",
    "            resp.body = po\n",
    "        except:\n",
    "            resp.body = json.dumps({'status': 'Failure', 'message' : 'Error occurred'})\n",
    "            resp.status = falcon.HTTP_500\n",
    "            raise falcon.HTTPInternalServerError('Internal Server Error', 'Predict failed! ')\n",
    "        '''\n",
    "        if po.bo.status == 'Success':\n",
    "            resp.status = falcon.HTTP_200\n",
    "            resp.body = (str(po.values))\n",
    "        elif po.bo.status == 'Failure':\n",
    "            resp.body = json.dumps({'status': 'Failure', 'message' : 'Error occurred'})\n",
    "            resp.status = falcon.HTTP_500\n",
    "            raise falcon.HTTPInternalServerError('Internal Server Error', 'Predict failed! ')       \n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leepand/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11dfe95d0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/model_resources/TensorflowMnistExample_1.0.0/', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "from model_loader import ModelLoader\n",
    "\n",
    "import yaml\n",
    "conf='config/AIFlyapi_config.yaml'\n",
    "with open(conf, 'rt') as f:\n",
    "    client_config = yaml.load(f.read())\n",
    "\n",
    "\n",
    "model_list=[[\"HelloWorldExample\", \"1.0.0\"],[\"HelloAI2\", \"1.0.0\"],[\"TensorflowMnistExample\",\"1.0.0\"],['fib_model',\"1.0.2\"]]\n",
    "model_loader = ModelLoader(client_config)\n",
    "models_to_load=model_list\n",
    "models_loaded = model_loader.get_models_from_list(models_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('HelloAI2', '1.0.0'): <__main__.HelloWorldModel instance at 0x107f07f38>,\n",
       " ('HelloWorldExample',\n",
       "  '1.0.0'): <__main__.HelloWorldModel instance at 0x107f07e60>,\n",
       " ('TensorflowMnistExample',\n",
       "  '1.0.0'): <TensorflowMnistExample_1_0_0.TensorflowMnistEstimator instance at 0x107dd40e0>,\n",
       " ('fib_model', '1.0.2'): <fib_model_1_0_2.fib instance at 0x11dff7878>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id, model_version=\"fib_model\",\"1.0.2\"\n",
    "import json\n",
    "key = (model_id, model_version)\n",
    "curr_model = models_loaded[key]\n",
    "data = {'n':2}\n",
    "#json_payload = json.dumps(data)\n",
    "#json_payload = json.loads(data)\n",
    "#json_payload={'n':2}\n",
    "output = curr_model.predict(2)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Prediction: \"Hello World\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from AIFlysdk.AIFlyApi import AIFlyApi\n",
    "class HelloWorldModel:\n",
    "    def predict(self, input):\n",
    "        return json.dumps(\"Hello World\")\n",
    "\n",
    "model = HelloWorldModel()\n",
    "print \"Local Prediction:\", model.predict(None)\n",
    "y=AIFlyApi(config='config/AIFlyapi_config.yaml')\n",
    "y.publish_model(model, \"HelloWorldExample\", \"1.0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "y=AIFlyApi(config='config/AIFlyapi_config.yaml')\n",
    "dir_path='/Users/leepand/Downloads/BigRLab_APIs/demoday_fs/web_lab/AIaas/AIserver/AIaasFly/AIFlysdk'\n",
    "y.publish_asm_model(path_to_prediction_module=dir_path+'/model/pred/fib.py',\n",
    "            path_to_model_resources_dir=dir_path+'/model/model',\n",
    "            model_id=\"fib_model\",\n",
    "                    model_version=\"1.0.2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
